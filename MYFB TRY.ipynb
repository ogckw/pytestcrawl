{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "FBID = '559501364202094'\n",
    "FBSECRET = 'c617b0fec4a6087cf86eef57b3561297'\n",
    "ACCESSTOKEN = '{}|{}'.format(FBID, FBSECRET)\n",
    "res = requests.get('https://graph.facebook.com/v2.4/tsaiingwen/posts?access_token={}&format=json&limit=100&method=get'.format(ACCESSTOKEN))\n",
    "jd = json.loads(res.text)\n",
    "ary = []\n",
    "for entry in jd['data']:\n",
    "    if 'message' in entry:\n",
    "        ary.append(entry['message'])\n",
    "#print jd['paging']['next']\n",
    "while  'paging' in jd and 'next' in jd['paging']:\n",
    "    next_url = jd['paging']['next']\n",
    "    res_next = requests.get(next_url)\n",
    "    jd = json.loads(res_next.text)\n",
    "    \n",
    "    for entry in jd['data']:\n",
    "        if 'message' in entry:\n",
    "            ary.append(entry['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.add_word('意義')\n",
    "jieba.add_word('民進黨')\n",
    "jieba.add_word('蔡英文')\n",
    "jieba.add_word('兩岸關係')\n",
    "dic_word = {}\n",
    "for entry in ary:\n",
    "    for w in ' ' .join(jieba.cut(entry)).split():\n",
    "        if w not in dic_word:\n",
    "            if len(w) >=2 :\n",
    "                dic_word[w] = 1\n",
    "        else:\n",
    "            dic_word[w] = dic_word[w] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_x = sorted(dic_word.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我們 3004\n",
      "台灣 2516\n",
      "社會 1044\n",
      "大家 1037\n",
      "政府 1012\n",
      "人民 999\n",
      "一個 880\n",
      "希望 761\n",
      "民主 725\n",
      "民進黨 708\n",
      "發展 674\n",
      "一起 672\n",
      "未來 647\n",
      "國家 607\n",
      "這個 554\n",
      "可以 541\n",
      "政治 537\n",
      "經濟 517\n",
      "問題 506\n",
      "他們 503\n",
      "自己 494\n",
      "蔡英文 490\n",
      "就是 426\n",
      "支持 383\n",
      "必須 382\n",
      "總統 381\n",
      "地方 377\n",
      "http 371\n",
      "力量 363\n",
      "朋友 361\n",
      "今天 361\n",
      "選舉 353\n",
      "政策 349\n",
      "努力 347\n",
      "改變 342\n",
      "文化 337\n",
      "重要 317\n",
      "共同 313\n",
      "改革 313\n",
      "生活 311\n",
      "應該 308\n",
      "產業 307\n",
      "看到 300\n",
      "中國 297\n",
      "新北市 297\n",
      "需要 291\n",
      "這些 289\n",
      "因為 288\n",
      "推動 283\n",
      "不是 273\n",
      "責任 272\n",
      "這是 271\n",
      "執政 262\n",
      "開始 253\n",
      "兩岸 251\n",
      "工作 249\n",
      "輕人 245\n",
      "以及 242\n",
      "機會 235\n",
      "照顧 233\n",
      "很多 233\n",
      "如果 232\n",
      "小英 226\n",
      "這樣 224\n",
      "成為 222\n",
      "許多 219\n",
      "幸福 214\n",
      "沒有 214\n",
      "自由 214\n",
      "不能 208\n",
      "活動 208\n",
      "政黨 206\n",
      "現在 205\n",
      "教育 204\n",
      "期待 201\n",
      "資源 200\n",
      "主席 199\n",
      "過程 196\n",
      "歷史 196\n",
      "謝謝 193\n",
      "提供 193\n",
      "環境 192\n",
      "進行 191\n",
      "提出 190\n",
      "這裡 190\n",
      "歡迎 186\n",
      "孩子 186\n",
      "一定 185\n",
      "gl 183\n",
      "goo 183\n",
      "已經 183\n",
      "解決 182\n",
      "國民黨 180\n",
      "價值 180\n",
      "方式 178\n",
      "不同 178\n",
      "土地 176\n",
      "非常 174\n",
      "國際 173\n",
      "所有 169\n"
     ]
    }
   ],
   "source": [
    "for ele in sorted_x[0:100]:\n",
    "    print ele[0], ele[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jieba:\n",
      "\n",
      "NAME\n",
      "    jieba\n",
      "\n",
      "FILE\n",
      "    c:\\python27\\lib\\site-packages\\jieba\\__init__.py\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    _compat\n",
      "    analyse (package)\n",
      "    finalseg (package)\n",
      "    posseg (package)\n",
      "\n",
      "CLASSES\n",
      "    __builtin__.object\n",
      "        Tokenizer\n",
      "    \n",
      "    class Tokenizer(__builtin__.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dictionary=u'c:\\\\python27\\\\lib\\\\site-packages\\\\jieba\\\\dict.txt')\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  add_word(self, word, freq=None, tag=None)\n",
      "     |      Add a word to dictionary.\n",
      "     |      \n",
      "     |      freq and tag can be omitted, freq defaults to be a calculated value\n",
      "     |      that ensures the word can be cut out.\n",
      "     |  \n",
      "     |  calc(self, sentence, DAG, route)\n",
      "     |  \n",
      "     |  check_initialized(self)\n",
      "     |  \n",
      "     |  cut(self, sentence, cut_all=False, HMM=True)\n",
      "     |      The main function that segments an entire sentence that contains\n",
      "     |      Chinese characters into seperated words.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: The str(unicode) to be segmented.\n",
      "     |          - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "     |          - HMM: Whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  cut_for_search(self, sentence, HMM=True)\n",
      "     |      Finer segmentation for search engines.\n",
      "     |  \n",
      "     |  del_word(self, word)\n",
      "     |      Convenient function for deleting a word.\n",
      "     |  \n",
      "     |  gen_pfdict(self, f_name)\n",
      "     |  \n",
      "     |  get_DAG(self, sentence)\n",
      "     |  \n",
      "     |  get_abs_path_dict(self)\n",
      "     |  \n",
      "     |  initialize(self, dictionary=None)\n",
      "     |  \n",
      "     |  lcut(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  lcut_for_search(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  load_userdict(self, f)\n",
      "     |      Load personalized dict to improve detect rate.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - f : A plain text file contains words and their ocurrences.\n",
      "     |      \n",
      "     |      Structure of dict file:\n",
      "     |      word1 freq1 word_type1\n",
      "     |      word2 freq2 word_type2\n",
      "     |      ...\n",
      "     |      Word type may be ignored\n",
      "     |  \n",
      "     |  set_dictionary(self, dictionary_path)\n",
      "     |  \n",
      "     |  suggest_freq(self, segment, tune=False)\n",
      "     |      Suggest word frequency to force the characters in a word to be\n",
      "     |      joined or splitted.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - segment : The segments that the word is expected to be cut into,\n",
      "     |                      If the word should be treated as a whole, use a str.\n",
      "     |          - tune : If True, tune the word frequency.\n",
      "     |      \n",
      "     |      Note that HMM may affect the final result. If the result doesn't change,\n",
      "     |      set HMM=False.\n",
      "     |  \n",
      "     |  tokenize(self, unicode_sentence, mode=u'default', HMM=True)\n",
      "     |      Tokenize a sentence and yields tuples of (word, start, end)\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: the str(unicode) to be segmented.\n",
      "     |          - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "     |          - HMM: whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    add_word(self, word, freq=None, tag=None) method of Tokenizer instance\n",
      "        Add a word to dictionary.\n",
      "        \n",
      "        freq and tag can be omitted, freq defaults to be a calculated value\n",
      "        that ensures the word can be cut out.\n",
      "    \n",
      "    calc(self, sentence, DAG, route) method of Tokenizer instance\n",
      "    \n",
      "    cut(self, sentence, cut_all=False, HMM=True) method of Tokenizer instance\n",
      "        The main function that segments an entire sentence that contains\n",
      "        Chinese characters into seperated words.\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: The str(unicode) to be segmented.\n",
      "            - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "            - HMM: Whether to use the Hidden Markov Model.\n",
      "    \n",
      "    cut_for_search(self, sentence, HMM=True) method of Tokenizer instance\n",
      "        Finer segmentation for search engines.\n",
      "    \n",
      "    del_word(self, word) method of Tokenizer instance\n",
      "        Convenient function for deleting a word.\n",
      "    \n",
      "    disable_parallel()\n",
      "    \n",
      "    enable_parallel(processnum=None)\n",
      "        Change the module's `cut` and `cut_for_search` functions to the\n",
      "        parallel version.\n",
      "        \n",
      "        Note that this only works using dt, custom Tokenizer\n",
      "        instances are not supported.\n",
      "    \n",
      "    get_DAG(self, sentence) method of Tokenizer instance\n",
      "    \n",
      "    get_FREQ lambda k, d=None\n",
      "    \n",
      "    get_abs_path_dict(self) method of Tokenizer instance\n",
      "    \n",
      "    initialize(self, dictionary=None) method of Tokenizer instance\n",
      "    \n",
      "    lcut(self, *args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    lcut_for_search(self, *args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    load_userdict(self, f) method of Tokenizer instance\n",
      "        Load personalized dict to improve detect rate.\n",
      "        \n",
      "        Parameter:\n",
      "            - f : A plain text file contains words and their ocurrences.\n",
      "        \n",
      "        Structure of dict file:\n",
      "        word1 freq1 word_type1\n",
      "        word2 freq2 word_type2\n",
      "        ...\n",
      "        Word type may be ignored\n",
      "    \n",
      "    log(...)\n",
      "        log(x[, base])\n",
      "        \n",
      "        Return the logarithm of x to the given base.\n",
      "        If the base not specified, returns the natural logarithm (base e) of x.\n",
      "    \n",
      "    md5 = openssl_md5(...)\n",
      "        Returns a md5 hash object; optionally initialized with a string\n",
      "    \n",
      "    setLogLevel(log_level)\n",
      "    \n",
      "    set_dictionary(self, dictionary_path) method of Tokenizer instance\n",
      "    \n",
      "    suggest_freq(self, segment, tune=False) method of Tokenizer instance\n",
      "        Suggest word frequency to force the characters in a word to be\n",
      "        joined or splitted.\n",
      "        \n",
      "        Parameter:\n",
      "            - segment : The segments that the word is expected to be cut into,\n",
      "                        If the word should be treated as a whole, use a str.\n",
      "            - tune : If True, tune the word frequency.\n",
      "        \n",
      "        Note that HMM may affect the final result. If the result doesn't change,\n",
      "        set HMM=False.\n",
      "    \n",
      "    tokenize(self, unicode_sentence, mode=u'default', HMM=True) method of Tokenizer instance\n",
      "        Tokenize a sentence and yields tuples of (word, start, end)\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: the str(unicode) to be segmented.\n",
      "            - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "            - HMM: whether to use the Hidden Markov Model.\n",
      "\n",
      "DATA\n",
      "    DEFAULT_DICT = u'c:\\\\python27\\\\lib\\\\site-packages\\\\jieba\\\\dict.txt'\n",
      "    DICT_WRITING = {}\n",
      "    PY2 = True\n",
      "    __license__ = u'MIT'\n",
      "    __version__ = u'0.37'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    default_encoding = 'mbcs'\n",
      "    default_logger = <logging.Logger object>\n",
      "    dt = <Tokenizer dictionary=u'c:\\\\python27\\\\lib\\\\site-packages\\\\jieba\\\\...\n",
      "    log_console = <logging.StreamHandler object>\n",
      "    pool = None\n",
      "    re_eng = <_sre.SRE_Pattern object>\n",
      "    re_han_cut_all = <_sre.SRE_Pattern object>\n",
      "    re_han_default = <_sre.SRE_Pattern object>\n",
      "    re_skip_cut_all = <_sre.SRE_Pattern object>\n",
      "    re_skip_default = <_sre.SRE_Pattern object>\n",
      "    string_types = (<type 'str'>, <type 'unicode'>)\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    user_word_tag_tab = {}\n",
      "\n",
      "VERSION\n",
      "    0.37\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "help(jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
